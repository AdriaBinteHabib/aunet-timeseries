{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25454,
     "status": "ok",
     "timestamp": 1748277983046,
     "user": {
      "displayName": "Arif Shakil",
      "userId": "16141773464943262306"
     },
     "user_tz": -360
    },
    "id": "WSHnIs-key4p",
    "outputId": "234e421a-d00c-4791-a089-f4e4fd32b039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/MyDrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/MyDrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5497,
     "status": "ok",
     "timestamp": 1748277998402,
     "user": {
      "displayName": "Arif Shakil",
      "userId": "16141773464943262306"
     },
     "user_tz": -360
    },
    "id": "0-pDTimse7tM"
   },
   "outputs": [],
   "source": [
    "\n",
    "# AUNET: Attention-Based Time Series Forecaster (TensorFlow / Keras)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers, callbacks\n",
    "\n",
    "class AUNET:\n",
    "    def __init__(self, input_length=30, forecast_horizon=7, feature_dim=None,\n",
    "                 lr=0.000352, dropout_rate=0.261, hidden_size=128, num_heads=8,\n",
    "                 patience=10, batch_size=32, epochs=50, verbose=1):\n",
    "        self.input_length = input_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.feature_dim = feature_dim\n",
    "        self.lr = lr\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "\n",
    "    def _build_model(self):\n",
    "        inputs = layers.Input(shape=(self.input_length, self.feature_dim))\n",
    "        x = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.hidden_size, dropout=self.dropout_rate)(inputs, inputs)\n",
    "        for _ in range(4):\n",
    "            x = layers.Dense(self.hidden_size, activation=\"relu\", kernel_regularizer=regularizers.l2(1.95e-6))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = layers.Dropout(self.dropout_rate)(x)\n",
    "        x = layers.Dense(self.forecast_horizon)(x)\n",
    "        outputs = layers.Flatten()(x)\n",
    "        model = models.Model(inputs, outputs)\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=self.lr), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y, validation_split=0.2):\n",
    "        if self.feature_dim is None:\n",
    "            self.feature_dim = X.shape[2]\n",
    "        self.model = self._build_model()\n",
    "        es = callbacks.EarlyStopping(monitor='val_loss', patience=self.patience, restore_best_weights=True)\n",
    "        self.history = self.model.fit(X, y,\n",
    "                                      epochs=self.epochs,\n",
    "                                      batch_size=self.batch_size,\n",
    "                                      validation_split=validation_split,\n",
    "                                      callbacks=[es],\n",
    "                                      verbose=self.verbose)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def evaluate(self, X, y_true, scaler_y=None):\n",
    "        y_pred = self.predict(X)\n",
    "        if scaler_y:\n",
    "            y_pred = scaler_y.inverse_transform(y_pred)\n",
    "            y_true = scaler_y.inverse_transform(y_true)\n",
    "        mae = np.mean(np.abs(y_true - y_pred))\n",
    "        rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "        r2 = 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
    "        return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "    def plot_learning_curve(self):\n",
    "        if self.history:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(self.history.history['loss'], label='Train Loss')\n",
    "            if 'val_loss' in self.history.history:\n",
    "                plt.plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Learning Curve')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "    def plot_predictions(self, y_true, y_pred, scaler_y=None, title='Prediction vs Actual'):\n",
    "        if scaler_y:\n",
    "            y_pred = scaler_y.inverse_transform(y_pred)\n",
    "            y_true = scaler_y.inverse_transform(y_true)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(y_true.flatten(), label='Actual')\n",
    "        plt.plot(y_pred.flatten(), label='Predicted')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP2QJOzMKXYmQD/G/kruDO1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
